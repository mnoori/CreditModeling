{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42538, 145)\n",
      "(42538, 53)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#if we dont skip the first row, we will end up with a dataset with only one column.\n",
    "data=pd.read_csv('LoanStats3a.csv',low_memory=False,skiprows=1)\n",
    "print(data.shape)\n",
    "\n",
    "#pretty interesting size! let's see how can be decrease the size for removing the text columns, such as url\n",
    "data=data.drop(['url','desc'],axis=1)\n",
    "\n",
    "# I also remove the columns with more than 50% missing values:\n",
    "threshold=data.shape[0]/2\n",
    "data=data.dropna(thresh=threshold,axis=1)\n",
    "print(data.shape)\n",
    "\n",
    "#we will save the data in a seprate file, just in case we wanted to epxplore the original file later on.\n",
    "data.to_csv('loans_2007.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to take a deeper look at the data dictionary and find the columns that we'd like to drop. The strategy is as follows. We will drop columns that:\n",
    "* leak information from the future (after the loan has already been funded)\n",
    "* don't affect a borrower's ability to pay back a loan (e.g. a randomly generated ID value by Lending Club)\n",
    "* formatted poorly and need to be cleaned up\n",
    "* require more data or a lot of processing to turn into a useful feature\n",
    "* contain redundant information\n",
    "\n",
    "For this purpose, I have devided the columns in 3 sets, and then have looked at the data dictionary. The list of columns that will be droped are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loan_amnt', 'term', 'int_rate', 'installment', 'emp_length',\n",
       "       'home_ownership', 'annual_inc', 'verification_status', 'loan_status',\n",
       "       'pymnt_plan', 'purpose', 'title', 'addr_state', 'dti', 'delinq_2yrs',\n",
       "       'earliest_cr_line', 'inq_last_6mths', 'open_acc', 'pub_rec',\n",
       "       'revol_bal', 'revol_util', 'total_acc', 'initial_list_status',\n",
       "       'last_credit_pull_d', 'collections_12_mths_ex_med', 'policy_code',\n",
       "       'application_type', 'acc_now_delinq', 'chargeoff_within_12_mths',\n",
       "       'delinq_amnt', 'pub_rec_bankruptcies', 'tax_liens', 'hardship_flag',\n",
       "       'disbursement_method', 'debt_settlement_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_cols=[\"funded_amnt\", \"funded_amnt_inv\", \"grade\", \"sub_grade\", \"emp_title\", \"issue_d\",\n",
    "           \"zip_code\", \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_prncp\",\n",
    "          \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \"collection_recovery_fee\", \"last_pymnt_d\", \"last_pymnt_amnt\"]\n",
    "\n",
    "\n",
    "data=data.drop(drop_cols,axis=1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target variable\n",
    "\n",
    "The target variable is `loan status`, as we want to see if we can predict the payability of a loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fully Paid                                             34116\n",
       "Charged Off                                             5670\n",
       "Does not meet the credit policy. Status:Fully Paid      1988\n",
       "Does not meet the credit policy. Status:Charged Off      761\n",
       "Name: loan_status, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target='loan_status'\n",
    "data[target].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two values for target variable do not exist in the data dictionary. Here's when background research comes in handy. Those two values represent the situation that the loan has been approved before, but it does not meet the new requirement. On the hand, we're interested to know whether someone will pay it's loan off or not. In this case, we can drop those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data[(data[target]=='Fully Paid')|(data[target]=='Charged Off')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I'd like to replace the text values with numbers, I will use the mapping ability of pd.replace() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map={\n",
    "    'loan_status':{\n",
    "        'Fully Paid':1,\n",
    "        'Charged Off':0,\n",
    "    }\n",
    "}\n",
    "data=data.replace(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    34116\n",
       "0     5670\n",
       "Name: loan_status, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[target].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the number of unique values in each column. We'd like to drop any column that has only one unique value, as it will not impact the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39786, 24)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unique_cols=[]\n",
    "for col in data.columns:\n",
    "    if len(data[col].value_counts().tolist())==1:\n",
    "        one_unique_cols.append(col)\n",
    "\n",
    "data=data.drop(one_unique_cols,axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the feature and feature selection\n",
    "\n",
    "My strategy is to first to deal with missing values, then convert categorical features to numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pub_rec_bankruptcies    697\n",
       "revol_util               50\n",
       "title                    10\n",
       "last_credit_pull_d        2\n",
       "debt_settlement_flag      0\n",
       "purpose                   0\n",
       "term                      0\n",
       "int_rate                  0\n",
       "installment               0\n",
       "emp_length                0\n",
       "home_ownership            0\n",
       "annual_inc                0\n",
       "verification_status       0\n",
       "loan_status               0\n",
       "addr_state                0\n",
       "dti                       0\n",
       "delinq_2yrs               0\n",
       "earliest_cr_line          0\n",
       "inq_last_6mths            0\n",
       "open_acc                  0\n",
       "pub_rec                   0\n",
       "revol_bal                 0\n",
       "total_acc                 0\n",
       "loan_amnt                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape[0])\n",
    "data.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are not alot! let's replace the null values with mean of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amnt                0\n",
      "term                     0\n",
      "int_rate                 0\n",
      "installment              0\n",
      "emp_length               0\n",
      "home_ownership           0\n",
      "annual_inc               0\n",
      "verification_status      0\n",
      "loan_status              0\n",
      "purpose                  0\n",
      "title                   10\n",
      "addr_state               0\n",
      "dti                      0\n",
      "delinq_2yrs              0\n",
      "earliest_cr_line         0\n",
      "inq_last_6mths           0\n",
      "open_acc                 0\n",
      "pub_rec                  0\n",
      "revol_bal                0\n",
      "revol_util              50\n",
      "total_acc                0\n",
      "last_credit_pull_d       2\n",
      "pub_rec_bankruptcies     0\n",
      "debt_settlement_flag     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data=data.fillna(data.mean())\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason the remaining null values do not change is that those columns are Objects. Let's remove those rows that contain missing values for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### dealing with object columns\n",
    "First let's store them in a seperate dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "term                      36 months\n",
       "int_rate                     10.65%\n",
       "emp_length                10+ years\n",
       "home_ownership                 RENT\n",
       "verification_status        Verified\n",
       "purpose                 credit_card\n",
       "title                      Computer\n",
       "addr_state                       AZ\n",
       "earliest_cr_line           Jan-1985\n",
       "revol_util                    83.7%\n",
       "last_credit_pull_d         Dec-2017\n",
       "debt_settlement_flag              N\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_cols=data.select_dtypes(include=['object'])\n",
    "object_cols.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of unique variables in these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values of term are\n",
      " 36 months    29041\n",
      " 60 months    10683\n",
      "Name: term, dtype: int64\n",
      "unique values of int_rate are\n",
      " 10.99%    958\n",
      " 13.49%    831\n",
      " 11.49%    824\n",
      "  7.51%    787\n",
      "  7.88%    725\n",
      "  7.49%    656\n",
      " 11.71%    609\n",
      "  9.99%    603\n",
      "  7.90%    582\n",
      "  5.42%    573\n",
      " 11.99%    535\n",
      " 12.69%    492\n",
      " 10.37%    470\n",
      " 12.99%    449\n",
      "  6.03%    447\n",
      "  8.49%    445\n",
      " 12.42%    443\n",
      " 10.65%    435\n",
      "  5.79%    410\n",
      "  8.90%    402\n",
      "  7.29%    397\n",
      "  6.62%    396\n",
      " 11.86%    391\n",
      " 14.27%    389\n",
      " 10.59%    381\n",
      "  9.63%    378\n",
      "  9.91%    377\n",
      "  5.99%    347\n",
      "  7.14%    340\n",
      "  6.99%    336\n",
      "          ... \n",
      " 15.83%      2\n",
      " 17.15%      2\n",
      " 15.01%      2\n",
      " 14.88%      2\n",
      " 21.82%      2\n",
      " 20.20%      2\n",
      " 17.03%      2\n",
      " 15.07%      2\n",
      " 14.25%      2\n",
      " 22.94%      2\n",
      " 14.62%      2\n",
      " 18.36%      1\n",
      " 10.64%      1\n",
      " 18.72%      1\n",
      " 16.33%      1\n",
      " 16.20%      1\n",
      " 16.01%      1\n",
      " 17.54%      1\n",
      " 17.44%      1\n",
      " 16.15%      1\n",
      " 21.48%      1\n",
      " 14.67%      1\n",
      " 24.59%      1\n",
      " 22.64%      1\n",
      " 17.34%      1\n",
      " 16.96%      1\n",
      " 24.40%      1\n",
      " 17.46%      1\n",
      " 20.52%      1\n",
      " 16.71%      1\n",
      "Name: int_rate, Length: 371, dtype: int64\n",
      "unique values of emp_length are\n",
      "10+ years    8897\n",
      "< 1 year     4576\n",
      "2 years      4389\n",
      "3 years      4094\n",
      "4 years      3435\n",
      "5 years      3279\n",
      "1 year       3240\n",
      "6 years      2227\n",
      "7 years      1771\n",
      "8 years      1483\n",
      "9 years      1259\n",
      "n/a          1074\n",
      "Name: emp_length, dtype: int64\n",
      "unique values of home_ownership are\n",
      "RENT        18881\n",
      "MORTGAGE    17688\n",
      "OWN          3056\n",
      "OTHER          96\n",
      "NONE            3\n",
      "Name: home_ownership, dtype: int64\n",
      "unique values of verification_status are\n",
      "Not Verified       16890\n",
      "Verified           12833\n",
      "Source Verified    10001\n",
      "Name: verification_status, dtype: int64\n",
      "unique values of purpose are\n",
      "debt_consolidation    18661\n",
      "credit_card            5134\n",
      "other                  3985\n",
      "home_improvement       2980\n",
      "major_purchase         2182\n",
      "small_business         1827\n",
      "car                    1549\n",
      "wedding                 947\n",
      "medical                 693\n",
      "moving                  581\n",
      "house                   382\n",
      "vacation                380\n",
      "educational             320\n",
      "renewable_energy        103\n",
      "Name: purpose, dtype: int64\n",
      "unique values of title are\n",
      "Debt Consolidation                                2189\n",
      "Debt Consolidation Loan                           1732\n",
      "Personal Loan                                      661\n",
      "Consolidation                                      516\n",
      "debt consolidation                                 508\n",
      "Home Improvement                                   357\n",
      "Credit Card Consolidation                          357\n",
      "Debt consolidation                                 334\n",
      "Small Business Loan                                329\n",
      "Credit Card Loan                                   319\n",
      "Personal                                           309\n",
      "Consolidation Loan                                 256\n",
      "Home Improvement Loan                              249\n",
      "personal loan                                      234\n",
      "personal                                           220\n",
      "Loan                                               213\n",
      "Wedding Loan                                       209\n",
      "consolidation                                      205\n",
      "Car Loan                                           204\n",
      "Other Loan                                         192\n",
      "Credit Card Payoff                                 155\n",
      "Wedding                                            154\n",
      "Major Purchase Loan                                145\n",
      "Credit Card Refinance                              144\n",
      "Consolidate                                        127\n",
      "Medical                                            122\n",
      "Credit Card                                        119\n",
      "home improvement                                   112\n",
      "My Loan                                             94\n",
      "Credit Cards                                        93\n",
      "                                                  ... \n",
      "blazer                                               1\n",
      "Moving soon                                          1\n",
      "Full Consolidation                                   1\n",
      "Paying off car loan and one mini loan                1\n",
      "HELP MY DEBT                                         1\n",
      "Engagement Rign                                      1\n",
      "looking for low apr loan                             1\n",
      "Three Long Years                                     1\n",
      "credit card fix                                      1\n",
      "home improvement for retirement home                 1\n",
      "Education loan for Masters                           1\n",
      "Repairs and expenses                                 1\n",
      "Payoff HELOC to refi house                           1\n",
      "Pay of House                                         1\n",
      "AB Consolidation Loan                                1\n",
      "DIY Home Improvement                                 1\n",
      "Business Advance Loan                                1\n",
      "Jessica's Ring                                       1\n",
      "Become Debt Free in 5 Years                          1\n",
      "Looking to buy that diamond for my future wife       1\n",
      "Paying off credit card for a lower rate              1\n",
      "Debt_Merger-2011                                     1\n",
      "Paying off my Credit Cards                           1\n",
      "Clean Slate for Graphic Designer                     1\n",
      "equine purchase                                      1\n",
      "Say No To Bugs! - Pool Screen Cage                   1\n",
      "kill chase                                           1\n",
      "Credit card refinance and conslodation               1\n",
      "Consolidate monthly payments                         1\n",
      "Debt Consolidation and Security                      1\n",
      "Name: title, Length: 19610, dtype: int64\n",
      "unique values of addr_state are\n",
      "CA    7095\n",
      "NY    3815\n",
      "FL    2869\n",
      "TX    2729\n",
      "NJ    1850\n",
      "IL    1524\n",
      "PA    1515\n",
      "VA    1407\n",
      "GA    1399\n",
      "MA    1343\n",
      "OH    1221\n",
      "MD    1053\n",
      "AZ     878\n",
      "WA     841\n",
      "CO     791\n",
      "NC     788\n",
      "CT     754\n",
      "MI     722\n",
      "MO     685\n",
      "MN     613\n",
      "NV     497\n",
      "SC     472\n",
      "WI     459\n",
      "AL     451\n",
      "OR     450\n",
      "LA     436\n",
      "KY     327\n",
      "OK     299\n",
      "KS     271\n",
      "UT     259\n",
      "AR     245\n",
      "DC     212\n",
      "RI     199\n",
      "NM     190\n",
      "WV     177\n",
      "HI     173\n",
      "NH     172\n",
      "DE     113\n",
      "MT      85\n",
      "WY      83\n",
      "AK      81\n",
      "SD      63\n",
      "VT      54\n",
      "MS      19\n",
      "TN      17\n",
      "IN       9\n",
      "ID       6\n",
      "NE       5\n",
      "IA       5\n",
      "ME       3\n",
      "Name: addr_state, dtype: int64\n",
      "unique values of earliest_cr_line are\n",
      "Nov-1998    371\n",
      "Oct-1999    366\n",
      "Dec-1998    349\n",
      "Oct-2000    345\n",
      "Dec-1997    328\n",
      "Nov-2000    320\n",
      "Nov-1999    319\n",
      "Oct-1998    306\n",
      "Sep-2000    306\n",
      "Nov-1997    299\n",
      "Dec-1995    295\n",
      "Dec-1999    290\n",
      "Jan-2000    284\n",
      "Dec-2000    284\n",
      "Jul-2000    273\n",
      "Nov-1996    272\n",
      "Dec-1996    272\n",
      "Aug-2000    271\n",
      "Sep-2001    266\n",
      "Oct-2001    266\n",
      "Oct-2002    261\n",
      "Sep-1999    258\n",
      "Aug-1998    257\n",
      "Apr-2000    255\n",
      "May-2000    254\n",
      "Aug-1999    251\n",
      "Oct-1997    249\n",
      "Sep-1998    247\n",
      "Jan-1999    247\n",
      "Oct-2003    245\n",
      "           ... \n",
      "Apr-1969      1\n",
      "Jun-1967      1\n",
      "Nov-1965      1\n",
      "Sep-1965      1\n",
      "Apr-1966      1\n",
      "Oct-2008      1\n",
      "Oct-1964      1\n",
      "Dec-1950      1\n",
      "Oct-1954      1\n",
      "Nov-1961      1\n",
      "Jul-1964      1\n",
      "Nov-1962      1\n",
      "Feb-1967      1\n",
      "Dec-1961      1\n",
      "Oct-1969      1\n",
      "Jun-1959      1\n",
      "Dec-1965      1\n",
      "Dec-1966      1\n",
      "Oct-1968      1\n",
      "May-1967      1\n",
      "Sep-1956      1\n",
      "Jun-1968      1\n",
      "Nov-1954      1\n",
      "Nov-1968      1\n",
      "Aug-1967      1\n",
      "Feb-1966      1\n",
      "Sep-1963      1\n",
      "Dec-1963      1\n",
      "Mar-1963      1\n",
      "Jun-1972      1\n",
      "Name: earliest_cr_line, Length: 526, dtype: int64\n",
      "unique values of revol_util are\n",
      "0%        980\n",
      "0.2%       63\n",
      "63%        62\n",
      "40.7%      59\n",
      "66.7%      58\n",
      "0.1%       58\n",
      "66.6%      57\n",
      "46.4%      57\n",
      "70.4%      57\n",
      "31.2%      57\n",
      "61%        57\n",
      "37.6%      56\n",
      "65.9%      56\n",
      "25.5%      55\n",
      "49.8%      55\n",
      "48.9%      55\n",
      "57.4%      55\n",
      "64.8%      55\n",
      "35.3%      54\n",
      "78.7%      54\n",
      "51.6%      54\n",
      "75.5%      54\n",
      "32%        54\n",
      "64.6%      54\n",
      "46.6%      54\n",
      "68.6%      54\n",
      "51.1%      54\n",
      "89.5%      54\n",
      "27.2%      54\n",
      "76.6%      54\n",
      "         ... \n",
      "21.92%      1\n",
      "9.34%       1\n",
      "36.94%      1\n",
      "8.01%       1\n",
      "36.88%      1\n",
      "18.82%      1\n",
      "33.39%      1\n",
      "29.77%      1\n",
      "6.75%       1\n",
      "58.77%      1\n",
      "8.58%       1\n",
      "49.69%      1\n",
      "17.78%      1\n",
      "47.36%      1\n",
      "7.43%       1\n",
      "39.95%      1\n",
      "34.89%      1\n",
      "24.66%      1\n",
      "49.63%      1\n",
      "24.65%      1\n",
      "57.56%      1\n",
      "0.49%       1\n",
      "0.05%       1\n",
      "10.61%      1\n",
      "25.33%      1\n",
      "8.49%       1\n",
      "37.73%      1\n",
      "10.08%      1\n",
      "43.61%      1\n",
      "12.42%      1\n",
      "Name: revol_util, Length: 1089, dtype: int64\n",
      "unique values of last_credit_pull_d are\n",
      "Dec-2017    8999\n",
      "Oct-2016    3992\n",
      "Nov-2017     908\n",
      "Oct-2017     822\n",
      "Feb-2017     788\n",
      "Sep-2017     681\n",
      "Aug-2017     655\n",
      "Feb-2013     581\n",
      "Mar-2016     510\n",
      "Jul-2017     502\n",
      "Jun-2017     471\n",
      "Mar-2013     434\n",
      "May-2017     427\n",
      "Mar-2017     419\n",
      "Dec-2016     419\n",
      "Nov-2016     404\n",
      "Jan-2017     402\n",
      "Sep-2014     388\n",
      "Jul-2014     384\n",
      "Mar-2014     384\n",
      "Dec-2014     383\n",
      "Apr-2017     380\n",
      "Apr-2016     379\n",
      "Aug-2014     376\n",
      "Feb-2016     369\n",
      "Feb-2014     364\n",
      "Oct-2014     363\n",
      "Jan-2014     352\n",
      "Jan-2016     344\n",
      "Apr-2014     336\n",
      "            ... \n",
      "Dec-2009      27\n",
      "Jan-2010      27\n",
      "Oct-2009      21\n",
      "Nov-2009      19\n",
      "Aug-2009      16\n",
      "Feb-2009      16\n",
      "Apr-2009      15\n",
      "Aug-2007      15\n",
      "Jul-2009      13\n",
      "Jun-2009      12\n",
      "Jan-2009      12\n",
      "Jun-2007      12\n",
      "May-2009      10\n",
      "Mar-2009       9\n",
      "Sep-2009       9\n",
      "Oct-2008       8\n",
      "Aug-2008       8\n",
      "Sep-2008       5\n",
      "Dec-2008       5\n",
      "Mar-2008       4\n",
      "Oct-2007       3\n",
      "Jan-2008       3\n",
      "Sep-2007       2\n",
      "Feb-2008       2\n",
      "May-2007       1\n",
      "Dec-2007       1\n",
      "May-2008       1\n",
      "Jun-2008       1\n",
      "Jul-2008       1\n",
      "Jul-2007       1\n",
      "Name: last_credit_pull_d, Length: 125, dtype: int64\n",
      "unique values of debt_settlement_flag are\n",
      "N    39577\n",
      "Y      147\n",
      "Name: debt_settlement_flag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for col in object_cols:\n",
    "    print('unique values of {0} are'.format(col))\n",
    "    print(data[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Term: 2 values\n",
    "* int-rate: could be converted to numerical\n",
    "* emp_lengh: could be converted to numerical\n",
    "* home_owenership: 4 values\n",
    "* Purpose and Title have common unique values\n",
    "* debt_settlement_flag: 2 values\n",
    "* last_credit_pulled has actually some values in the future (2011 and beyond)\n",
    "\n",
    "I'll drop the title column, it has some data quality issues including repetitive use of debt consolidation.\n",
    "Also, considering last_credit_pulled and earliest_cr_line will transform our problem to a time series problem. Therfore, we will drop them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping_dict = {\n",
    "    \"emp_length\": {\n",
    "        \"10+ years\": 10,\n",
    "        \"9 years\": 9,\n",
    "        \"8 years\": 8,\n",
    "        \"7 years\": 7,\n",
    "        \"6 years\": 6,\n",
    "        \"5 years\": 5,\n",
    "        \"4 years\": 4,\n",
    "        \"3 years\": 3,\n",
    "        \"2 years\": 2,\n",
    "        \"1 year\": 1,\n",
    "        \"< 1 year\": 0,\n",
    "        \"n/a\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "data = data.drop([\"last_credit_pull_d\", \"earliest_cr_line\", \"addr_state\", \"title\"], axis=1)\n",
    "data[\"int_rate\"] = data[\"int_rate\"].str.rstrip(\"%\").astype(\"float\")\n",
    "data[\"revol_util\"] = data[\"revol_util\"].str.rstrip(\"%\").astype(\"float\")\n",
    "data=data.replace(mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now encode the rest of categorical columns to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [\"home_ownership\", \"verification_status\", \"purpose\", \"term\",'debt_settlement_flag']\n",
    "prefix_dict={\n",
    "    'home_ownership':'home_own',\n",
    "    'verification_status':'veri_stat',\n",
    "    'purpose':'purp',\n",
    "    'term':'term',\n",
    "    'debt_settlement_flag':'settle'\n",
    "}\n",
    "dummy_df=pd.get_dummies(data[cat_columns],prefix=prefix_dict)\n",
    "dummy_df.head()\n",
    "data=pd.concat([data,dumy_df],axis=1)\n",
    "#dropping the original columns\n",
    "data=data.drop(cat_columns,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fitting\n",
    "\n",
    "First, it is important to notice the class imbalance. Fully paid cases are 6 times more than charged off cases, this might end up overfitting our model.\n",
    "\n",
    "Also, I plan to use false positive and true positive rates as error metrics. Flase positive indicates \"how many percentage of positive outcome does my model mis-predict?\" True Positive ratio says: \"How many percentage of positive outcome does my model predict right?\n",
    "\n",
    "We use logistic regression for this binary classification, becuase:\n",
    "* it's quick to train and we can iterate more quickly,\n",
    "* it's less prone to overfitting than more complex models like decision trees,\n",
    "* it's easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=data.columns.tolist()\n",
    "features.remove('loan_status')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpr is: 0.9991180880148162\n",
      "fpr is: 0.9994685562444642\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_predict, KFold\n",
    "\n",
    "lr = LogisticRegression()\n",
    "#passing on total number of elements (n), default number of folds are 3.\n",
    "kf = KFold(data[features].shape[0], random_state=1)\n",
    "\n",
    "predictions = cross_val_predict(lr, data[features], data[target], cv=kf)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "def tpr_fpr(predictions):\n",
    "    \n",
    "    # False positives.\n",
    "    fp_filter = (predictions == 1) & (data[\"loan_status\"] == 0)\n",
    "    fp = len(predictions[fp_filter])\n",
    "    \n",
    "    # True positives.\n",
    "    tp_filter = (predictions == 1) & (data[\"loan_status\"] == 1)\n",
    "    tp = len(predictions[tp_filter])\n",
    "    \n",
    "    # False negatives.\n",
    "    fn_filter = (predictions == 0) & (data[\"loan_status\"] == 1)\n",
    "    fn = len(predictions[fn_filter])\n",
    "    \n",
    "    # True negatives\n",
    "    tn_filter = (predictions == 0) & (data[\"loan_status\"] == 0)\n",
    "    tn = len(predictions[tn_filter])\n",
    "    \n",
    "    # Rates\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "\n",
    "    print('tpr is:',tpr)\n",
    "    print('fpr is:', fpr)\n",
    "\n",
    "          \n",
    "tpr_fpr(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too much of fpr! we should do something about class imbalance. We have the option of over and under sampling. But I chose to use penalizing the fully paid records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpr is: 0.6285092747743776\n",
      "fpr is: 0.5918511957484499\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(class_weight='balanced')\n",
    "#passing on total number of elements (n), default number of folds are 3.\n",
    "kf = KFold(data[features].shape[0], random_state=1)\n",
    "\n",
    "predictions = cross_val_predict(lr, data[features], data[target], cv=kf)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "tpr_fpr(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower fpr, but how about we modify penalizing manually. We can do this by passing a dictionary to weight_class parametr of logictic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpr is: 0.2034570949819208\n",
      "fpr is: 0.19238263950398582\n"
     ]
    }
   ],
   "source": [
    "penalty = {\n",
    "    0: 10,\n",
    "    1: 1\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(class_weight=penalty)\n",
    "#passing on total number of elements (n), default number of folds are 3.\n",
    "kf = KFold(data[features].shape[0], random_state=1)\n",
    "\n",
    "predictions = cross_val_predict(lr, data[features], data[target], cv=kf)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "tpr_fpr(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substaintial improvement in fpr, but with the cost of very low tpr. From a conservative investor standpoint, it is good that fpr is low, but we'd like to improve our overall accuracy.\n",
    "\n",
    "Now,let's use a random forest algorithm, as we have seen nonlinear relationship in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpr is: 0.5384072669547579\n",
      "fpr is: 0.5073516386182463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "penalty = {\n",
    "    0: 10,\n",
    "    1: 1\n",
    "}\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100, min_samples_leaf=25,class_weight=penalty)\n",
    "predictions = cross_val_predict(clf, data[features], data[target], cv=kf)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "tpr_fpr(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the model is not working properly. There are few ways to approach from here. Ensambling with other algorithms, tuning current algorithms, tweaking penalties, including the columns we already droped, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
